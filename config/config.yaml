store: './data/20e_fullvocab_512u/'
preprocessing:
  experiment_size: 256 #use a subset of samples to perform a mock  run
  tokenizer_dir: './data/tokenizer.json'
  images_dir: './data/Flicker8k_Images/'

  images_features_dir: './data/img_features/'
  karpathy_test_val_split_json: './data/annotations/dataset_flickr8k_karpathy.json'
  raw_pretrained_embedding_dir: 'data/annotations/glove.840B.300d.txt'

nn_params:
  checkpoint_dir: 'checkpoints/'
  pred_df_dir: 'derived_data/'
  loss_dir: 'derived_data/loss_per_epoch.csv'
  embedding_dim: 300
  BATCH_SIZE: 64
  BUFFER_SIZE: 1000
  EPOCHS: 20
  units: 512
  # vocab_size: 6000 #specify this param if you want to restrict training vocabulary to these many words
